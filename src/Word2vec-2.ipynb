{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2vec.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "nzgdYWHfdKaO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "10e6c4c1-d836-4057-d7b1-4423d67172c7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525531848931,
          "user_tz": -180,
          "elapsed": 1132869,
          "user": {
            "displayName": "Otello 47",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110050518292405487974"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b276189f-e033-42b0-b317-c08a1c225872\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-b276189f-e033-42b0-b317-c08a1c225872\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving array.npy to array.npy\n",
            "User uploaded file \"array.npy\" with length 111788428 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-wm52T51vx2c",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "97880d34-ddc7-4531-a14f-180736e2ce61",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525552266682,
          "user_tz": -180,
          "elapsed": 794,
          "user": {
            "displayName": "Otello 47",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110050518292405487974"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "import os\n",
        "PATH = os.getcwd()\n",
        "data_dir_list = os.listdir(PATH) # show files which can be used in notebook\n",
        "data_dir_list"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['datalab',\n",
              " '.local',\n",
              " '.ipython',\n",
              " 'test2.txt',\n",
              " 'test.txt',\n",
              " 'test3.txt',\n",
              " 'array2.txt',\n",
              " '.forever',\n",
              " '.config',\n",
              " '.nv',\n",
              " '.rnd',\n",
              " '.cache']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "ut939Hu6sKeD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('array2.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KbzxBm27vy0e",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69dcd8fc-0db4-4f98-d26b-dd77f4775ef7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525546983781,
          "user_tz": -180,
          "elapsed": 5026,
          "user": {
            "displayName": "Otello 47",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110050518292405487974"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.23.2)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6ZhiM5Qov1kX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# PyDrive reference:\n",
        "# https://googledrive.github.io/PyDrive/docs/build/html/index.html\n",
        "#http://pythonhosted.org/PyDrive/pydrive.html\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rxlQL5Cev5AU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def fiload(q):\n",
        "  \"\"\"loads file into notebook\n",
        "  q-string, name of a file in googledrive\"\"\"\n",
        "  for file1 in file_list:\n",
        "      if  file1 [\"title\"] == q:\n",
        "        title=file1 [\"title\"]\n",
        "        id=file1['id']\n",
        "        file_obj = drive.CreateFile({'id': id})\n",
        "        file_obj.GetContentFile(title)\n",
        "        print(\"File is loaded!\")\n",
        "        break\n",
        "      continue "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dIniR5Iov7SA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "b73171a9-1c31-4dfb-f4ed-6aea57d83e72",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525546989507,
          "user_tz": -180,
          "elapsed": 770,
          "user": {
            "displayName": "Otello 47",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110050518292405487974"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "file_list = drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList() #get list of all files in the root directory\n",
        "for file1 in file_list:\n",
        "  print('title: %s, id: %s' % (file1['title'], file1['id']))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: Word2vec.ipynb, id: 1nKn7937mMcHB3nzj0st0kmJqfRKbe7bu\n",
            "title: test3.txt, id: 1RQS6YAU32H1ULVkHmKdwOoO4ixxBRPtE\n",
            "title: test2.txt, id: 1OaD91jnJHRd4QhqjDTz7rNAAEe5EkYHq\n",
            "title: test.txt, id: 1w-fgsiP0vRm5jM67_CiudZQvNatP1SCy\n",
            "title: Try to detect litho-new labels _RGB.ipynb, id: 1MSK9GHUDrpc9sQHzGW6Yw1FFG2lpYuF6\n",
            "title: homework03_part2_vae.ipynb, id: 1KyRJ0UWQcYy7WSZOVPRTfBNNTrmtje76\n",
            "title: humanparsing.py, id: 1XERUrbi0FeP0LvJR5XjlI4z5GD-eOSEG\n",
            "title: lfw_dataset.py, id: 1347JBpZAfSd_0cn_U1UpYMEAHzzEIpXH\n",
            "title: cifar.py, id: 14kQ1ll-sBcTR8LirL6eNIJDc8FlB0oyE\n",
            "title: notmnist.py, id: 1aBL9iAo1a9a4thjZ_DQiusUiwFPFO4QS\n",
            "title: homework_part1.ipynb, id: 1em1B6xFoJ3QCMvVAJxZbA1Sl2htUiWHn\n",
            "title: Deep learning, id: 1lGgadZk68gbEx5qjRVZ9r41LdrKYtLNw\n",
            "title: homework_part1.ipynb, id: 1LHZxE_Qnmz0zeBPOEvZsoQoh2w_SLpAJ\n",
            "title: homework_differentiation.ipynb, id: 1dp1oYoR6GHbF8HJyOsos1ZSmfTXYFZN3\n",
            "title: data.rar, id: 1KKqp6ZvEp6jsgeHvmTjzFcSctLyb25Ch\n",
            "title: train_data_short_bw-512.npy, id: 1aqO7tr8n-HYm2_u4BFiUhrlftaPlKv9S\n",
            "title: train_data_short_bw-256.npy, id: 15W17Kh7E9RcU5GDvlPTJC1nts9wNCxVd\n",
            "title: train_data_short_bw-1024.npy, id: 1mgJLY-nYQL7XGPCEGilIPXuDWz8bFhcs\n",
            "title: train_data_short_rgb-512.npy, id: 1zwM8_Fot4ItCWKLGlSsAXfMuz8fMVnIp\n",
            "title: train_data_short_rgb-1024.npy, id: 1TMsEZyKXR-RUM4usYleyfpof0xKcaKMX\n",
            "title: train_data_short_rgb-256.npy, id: 1GTTKF6JcOfg4tdxBDHM6u6EgI42XR_1x\n",
            "title: Copy of Try to detect litho-new labels _RGB.ipynb, id: 1B4_i0GZ4VuLPyHpPTw9ScBa4VnM5zRhh\n",
            "title: train_data_bw-1024.npy, id: 1rpWLPDv4ivBR3mCJEECSOEDqPcXmz5dH\n",
            "title: train_data_rgb-1024.npy, id: 1B_OAtR1XFW2j7t7fWpDkDDeIacdPNe_v\n",
            "title: train_data_rgb-128.npy, id: 1hcNI9AQcd8lyP3PRn2ugI2bXGtEQ5Eop\n",
            "title: train_data_bw-512.npy, id: 1ywAoDZlGgjbKlHBipqFLmyoFFfmeXlMZ\n",
            "title: train_data_bw-256.npy, id: 10t1308sDOyDQL9MMp1skGf7cpYzWNB23\n",
            "title: train_data_bw-128.npy, id: 1ypNDOonnkmphKCFs4D9KZE55UL03KcgC\n",
            "title: short_train_data_rgb-128.npy, id: 100DtrkeXbjgOwrKGss2kk24_evhWPzz_\n",
            "title: train_data-256.npy, id: 1VYZYLDqKj2uZd0t64v5gQ_naP9Xt711k\n",
            "title: train_data_rgb-256.npy, id: 1ua1dGp3wSj2i14tcou6ggelrPVyg6HK3\n",
            "title: train_data_rgb-512.npy, id: 1ZAHjdrpeTSKTSP2rHyrkzt8i6uC4ip9j\n",
            "title: Sample upload.txt, id: 1a9mtJiVz5B757rbLhJqjk5woFgYQgANs\n",
            "title: train_data_rgb.npy, id: 1uz6pkbcaXpSXs1A8z1Ka0fFNtOjs2lk0\n",
            "title: Try to detect litho-new labels.ipynb, id: 1tcumnRIe8bpXDGEFlfEy9jCbbppzM2tw\n",
            "title: Tensorflow%20for%20Poets.ipynb, id: 1OPoXuyipMrODj99UeavdrPLHf-vFDiEo\n",
            "title: Colab Notebooks, id: 1ML2O2jQ6pq3MdUtd8CpMVM_yZ4wCQ-74\n",
            "title: EBSCO, id: 0B45GR7HkX0eURXRIRHl6azlSVWs\n",
            "title: Мои сохраненные места, id: 1mobgFjmh6UpXa1TkDjEJrIoNDy0\n",
            "title: База мгу, id: 1J3t0bfaT8d475wWlVcvStcictSw\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oBy-t6GowCBU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f27d9474-8b06-4866-a032-1a507dde02ad",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525546990925,
          "user_tz": -180,
          "elapsed": 1256,
          "user": {
            "displayName": "Otello 47",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110050518292405487974"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "name=\"test3.txt\"\n",
        "fiload(name)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File is loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pOWtifow0X_v",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jFl276c60hoU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b217cbf7-c2fa-4378-b1ec-c2816f479dc9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525546997861,
          "user_tz": -180,
          "elapsed": 2629,
          "user": {
            "displayName": "Otello 47",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110050518292405487974"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.23.2)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TGe2cabzdG6d",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#model\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class SkipGramModel(nn.Module):\n",
        "    \"\"\"Skip gram model of word2vec.\n",
        "\n",
        "    Attributes:\n",
        "        emb_size: Embedding size.\n",
        "        emb_dimention: Embedding dimention, typically from 50 to 500.\n",
        "        u_embedding: Embedding for center word.\n",
        "        v_embedding: Embedding for neibor words.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, emb_size, emb_dimension):\n",
        "        \"\"\"Initialize model parameters.\n",
        "\n",
        "        Apply for two embedding layers.\n",
        "        Initialize layer weight\n",
        "\n",
        "        Args:\n",
        "            emb_size: Embedding size.\n",
        "            emb_dimention: Embedding dimention, typically from 50 to 500.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.u_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
        "        self.v_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
        "        self.init_emb()\n",
        "\n",
        "    def init_emb(self):\n",
        "        \"\"\"Initialize embedding weight like word2vec.\n",
        "\n",
        "        The u_embedding is a uniform distribution in [-0.5/em_size, 0.5/emb_size], and the elements of v_embedding are zeroes.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        initrange = 0.5 / self.emb_dimension\n",
        "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        self.v_embeddings.weight.data.uniform_(-0, 0)\n",
        "\n",
        "    def forward(self, pos_u, pos_v, neg_v):\n",
        "        \"\"\"Forward process.\n",
        "\n",
        "        As pytorch designed, all variables must be batch format, so all input of this method is a list of word id.\n",
        "\n",
        "        Args:\n",
        "            pos_u: list of center word ids for positive word pairs.\n",
        "            pos_v: list of neibor word ids for positive word pairs.\n",
        "            neg_u: list of center word ids for negative word pairs.\n",
        "            neg_v: list of neibor word ids for negative word pairs.\n",
        "\n",
        "        Returns:\n",
        "            Loss of this process, a pytorch variable.\n",
        "        \"\"\"\n",
        "        emb_u = self.u_embeddings(pos_u)\n",
        "        emb_v = self.v_embeddings(pos_v)\n",
        "        score = torch.mul(emb_u, emb_v).squeeze()\n",
        "        score = torch.sum(score, dim=1)\n",
        "        score = F.logsigmoid(score)\n",
        "        neg_emb_v = self.v_embeddings(neg_v)\n",
        "        neg_score = torch.bmm(neg_emb_v, emb_u.unsqueeze(2)).squeeze()\n",
        "        neg_score = F.logsigmoid(-1 * neg_score)\n",
        "        return -1 * (torch.sum(score)+torch.sum(neg_score))\n",
        "\n",
        "    def save_embedding(self, id2word, file_name, use_cuda):\n",
        "        \"\"\"Save all embeddings to file.\n",
        "\n",
        "        As this class only record word id, so the map from id to word has to be transfered from outside.\n",
        "\n",
        "        Args:\n",
        "            id2word: map from word id to word.\n",
        "            file_name: file name.\n",
        "        Returns:\n",
        "            None.\n",
        "        \"\"\"\n",
        "        if use_cuda:\n",
        "            embedding = self.u_embeddings.weight.cpu().data.numpy()\n",
        "        else:\n",
        "            embedding = self.u_embeddings.weight.data.numpy()\n",
        "        fout = open(file_name, 'w')\n",
        "        fout.write('%d %d\\n' % (len(id2word), self.emb_dimension))\n",
        "        for wid, w in id2word.items():\n",
        "            e = embedding[wid]\n",
        "            e = ' '.join(map(lambda x: str(x), e))\n",
        "            fout.write('%s %s\\n' % (w, e))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model = SkipGramModel(100, 100)\n",
        "    id2word = dict()\n",
        "    for i in range(100):\n",
        "        id2word[i] = str(i)\n",
        "    model.save_embedding(id2word)\n",
        "\n",
        "\n",
        "#if __name__ == '__main__':\n",
        " #   test()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6dwM4uHudG6i",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#input data \n",
        "import numpy\n",
        "from collections import deque\n",
        "numpy.random.seed(12345)\n",
        "\n",
        "\n",
        "class InputData:\n",
        "    \"\"\"Store data for word2vec, such as word map, sampling table and so on.\n",
        "\n",
        "    Attributes:\n",
        "        word_frequency: Count of each word, used for filtering low-frequency words and sampling table\n",
        "        word2id: Map from word to word id, without low-frequency words.\n",
        "        id2word: Map from word id to word, without low-frequency words.\n",
        "        sentence_count: Sentence count in files.\n",
        "        word_count: Word count in files, without low-frequency words.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, file_name, min_count):\n",
        "        self.input_file_name = file_name\n",
        "        self.get_words(min_count)\n",
        "        self.word_pair_catch = deque()\n",
        "        self.init_sample_table()\n",
        "        print('Word Count: %d' % len(self.word2id))\n",
        "        print('Sentence Length: %d' % (self.sentence_length))\n",
        "\n",
        "    def get_words(self, min_count):\n",
        "        self.input_file = open(self.input_file_name)\n",
        "        self.sentence_length = 0\n",
        "        self.sentence_count = 0\n",
        "        word_frequency = dict()\n",
        "        for line in self.input_file:\n",
        "            self.sentence_count += 1\n",
        "            line = line.strip().split(' ')\n",
        "            self.sentence_length += len(line)\n",
        "            for w in line:\n",
        "                try:\n",
        "                    word_frequency[w] += 1\n",
        "                except:\n",
        "                    word_frequency[w] = 1\n",
        "        self.word2id = dict()\n",
        "        self.id2word = dict()\n",
        "        wid = 0\n",
        "        self.word_frequency = dict()\n",
        "        for w, c in word_frequency.items():\n",
        "            if c < min_count:\n",
        "                self.sentence_length -= c\n",
        "                continue\n",
        "            self.word2id[w] = wid\n",
        "            self.id2word[wid] = w\n",
        "            self.word_frequency[wid] = c\n",
        "            wid += 1\n",
        "        self.word_count = len(self.word2id)\n",
        "\n",
        "    def init_sample_table(self):\n",
        "        self.sample_table = []\n",
        "        sample_table_size = 1e8\n",
        "        pow_frequency = numpy.array(list(self.word_frequency.values()))**0.75\n",
        "        words_pow = sum(pow_frequency)\n",
        "        ratio = pow_frequency / words_pow\n",
        "        count = numpy.round(ratio * sample_table_size)\n",
        "        for wid, c in enumerate(count):\n",
        "            self.sample_table += [wid] * int(c)\n",
        "        self.sample_table = numpy.array(self.sample_table)\n",
        "\n",
        "    # @profile\n",
        "    def get_batch_pairs(self, batch_size, window_size):\n",
        "        while len(self.word_pair_catch) < batch_size:\n",
        "            sentence = self.input_file.readline()\n",
        "            if sentence is None or sentence == '':\n",
        "                self.input_file = open(self.input_file_name)\n",
        "                sentence = self.input_file.readline()\n",
        "            word_ids = []\n",
        "            for word in sentence.strip().split(' '):\n",
        "                try:\n",
        "                    word_ids.append(self.word2id[word])\n",
        "                except:\n",
        "                    continue\n",
        "            for i, u in enumerate(word_ids):\n",
        "                for j, v in enumerate(\n",
        "                        word_ids[max(i - window_size, 0):i + window_size]):\n",
        "                    assert u < self.word_count\n",
        "                    assert v < self.word_count\n",
        "                    if i == j:\n",
        "                        continue\n",
        "                    self.word_pair_catch.append((u, v))\n",
        "        batch_pairs = []\n",
        "        for _ in range(batch_size):\n",
        "            batch_pairs.append(self.word_pair_catch.popleft())\n",
        "        return batch_pairs\n",
        "\n",
        "    # @profile\n",
        "    def get_neg_v_neg_sampling(self, pos_word_pair, count):\n",
        "        neg_v = numpy.random.choice(\n",
        "            self.sample_table, size=(len(pos_word_pair), count)).tolist()\n",
        "        return neg_v\n",
        "\n",
        "    def evaluate_pair_count(self, window_size):\n",
        "        return self.sentence_length * (2 * window_size - 1) - (\n",
        "            self.sentence_count - 1) * (1 + window_size) * window_size\n",
        "\n",
        "\n",
        "def test():\n",
        "    a = InputData('./zhihu.txt')\n",
        "\n",
        "\n",
        "#if __name__ == '__main__':\n",
        " #   test()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dW3XwwGrdG6o",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "0db2492b-8bfe-43e8-eae6-cda50feb8abd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525547019768,
          "user_tz": -180,
          "elapsed": 19937,
          "user": {
            "displayName": "Otello 47",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110050518292405487974"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#w2vec\n",
        "import numpy\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "\n",
        "class Word2Vec:\n",
        "    def __init__(self,\n",
        "                 input_file_name=\"zhihu2.txt\",\n",
        "                 output_file_name=\"zhihuvec.txt\",\n",
        "                 emb_dimension=100,\n",
        "                 batch_size=50,\n",
        "                 window_size=5,\n",
        "                 iteration=1,\n",
        "                 initial_lr=0.025,\n",
        "                 min_count=5):\n",
        "        \"\"\"Initilize class parameters.\n",
        "\n",
        "        Args:\n",
        "            input_file_name: Name of a text data from file. Each line is a sentence splited with space.\n",
        "            output_file_name: Name of the final embedding file.\n",
        "            emb_dimention: Embedding dimention, typically from 50 to 500.\n",
        "            batch_size: The count of word pairs for one forward.\n",
        "            window_size: Max skip length between words.\n",
        "            iteration: Control the multiple training iterations.\n",
        "            initial_lr: Initial learning rate.\n",
        "            min_count: The minimal word frequency, words with lower frequency will be filtered.\n",
        "\n",
        "        Returns:\n",
        "            None.\n",
        "        \"\"\"\n",
        "        self.data = InputData(input_file_name, min_count)\n",
        "        self.output_file_name = output_file_name\n",
        "        self.emb_size = len(self.data.word2id)\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.batch_size = batch_size\n",
        "        self.window_size = window_size\n",
        "        self.iteration = iteration\n",
        "        self.initial_lr = initial_lr\n",
        "        self.skip_gram_model = SkipGramModel(self.emb_size, self.emb_dimension)\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        if self.use_cuda:\n",
        "            self.skip_gram_model.cuda()\n",
        "        self.optimizer = optim.SGD(\n",
        "            self.skip_gram_model.parameters(), lr=self.initial_lr)\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Multiple training.\n",
        "\n",
        "        Returns:\n",
        "            None.\n",
        "        \"\"\"\n",
        "        pair_count = self.data.evaluate_pair_count(self.window_size)\n",
        "        batch_count = self.iteration * pair_count / self.batch_size\n",
        "        process_bar = tqdm(range(int(batch_count)))\n",
        "        # self.skip_gram_model.save_embedding(\n",
        "        #     self.data.id2word, 'begin_embedding.txt', self.use_cuda)\n",
        "        for i in process_bar:\n",
        "            pos_pairs = self.data.get_batch_pairs(self.batch_size,\n",
        "                                                  self.window_size)\n",
        "            neg_v = self.data.get_neg_v_neg_sampling(pos_pairs, 5)\n",
        "            pos_u = [pair[0] for pair in pos_pairs]\n",
        "            pos_v = [pair[1] for pair in pos_pairs]\n",
        "\n",
        "            pos_u = Variable(torch.LongTensor(pos_u))\n",
        "            pos_v = Variable(torch.LongTensor(pos_v))\n",
        "            neg_v = Variable(torch.LongTensor(neg_v))\n",
        "            if self.use_cuda:\n",
        "                pos_u = pos_u.cuda()\n",
        "                pos_v = pos_v.cuda()\n",
        "                neg_v = neg_v.cuda()\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = self.skip_gram_model.forward(pos_u, pos_v, neg_v)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            process_bar.set_description(\"Loss: %0.8f, lr: %0.6f\" %\n",
        "                                        (loss.data[0],\n",
        "                                         self.optimizer.param_groups[0]['lr']))\n",
        "            if i * self.batch_size % 100000 == 0:\n",
        "                lr = self.initial_lr * (1.0 - 1.0 * i / batch_count)\n",
        "                for param_group in self.optimizer.param_groups:\n",
        "                    param_group['lr'] = lr\n",
        "        self.skip_gram_model.save_embedding(\n",
        "            self.data.id2word, self.output_file_name, self.use_cuda)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    w2v = Word2Vec(input_file_name=\"test3.txt\", output_file_name=\"array2.txt\")\n",
        "    w2v.train()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word Count: 321\n",
            "Sentence Length: 4172\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 135.35122681, lr: 0.025000: 100%|██████████| 750/750 [00:06<00:00, 110.79it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "E9dW3gBTdgW8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "5d4ed918-74be-4adc-909b-9bf90004fc2b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525442557735,
          "user_tz": -180,
          "elapsed": 2853,
          "user": {
            "displayName": "Otello 47",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110050518292405487974"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tqdm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/05/d95bda5a2d833be7593ac0d7eee502acf70d05a4d3a93ef474691a55c531/tqdm-4.23.2-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 2.0MB/s \n",
            "\u001b[?25hInstalling collected packages: tqdm\n",
            "Successfully installed tqdm-4.23.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uP5N9k-XdX7D",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ksxZiOwRdG6v",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "outputId": "c8dc88f3-db07-4da5-c3ad-617330ce8fd2",
        "executionInfo": {
          "status": "error",
          "timestamp": 1525552327974,
          "user_tz": -180,
          "elapsed": 34048,
          "user": {
            "displayName": "Otello 47",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110050518292405487974"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#test - didn't use this one\n",
        "import sys\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy\n",
        "f=open(\"test3.txt\")\n",
        "f.readline()\n",
        "all_embeddings=[]\n",
        "all_words=[]\n",
        "word2id=dict()\n",
        "for i,line in enumerate(f):\n",
        "    line=line.strip().split(' ')\n",
        "    word=line[0]\n",
        "    embedding=[float(x) for x in line[1:]]\n",
        "    assert len(embedding)==100\n",
        "    all_embeddings.append(embedding)\n",
        "    all_words.append(word)\n",
        "    word2id[word]=i\n",
        "all_embeddings=numpy.array(all_embeddings)\n",
        "while 1:\n",
        "    word=input('Word: ')\n",
        "    try:\n",
        "        wid=word2id[word]\n",
        "    except:\n",
        "        print('Cannot find this word')\n",
        "        continue\n",
        "    embedding=all_embeddings[wid:wid+1]\n",
        "    d = cosine_similarity(embedding, all_embeddings)[0]\n",
        "    d=zip(all_words, d)\n",
        "    d=sorted(d, key=lambda x:x[1], reverse=True)\n",
        "    for w in d[:10]:\n",
        "        if len(w[0])<2:\n",
        "            continue\n",
        "        print(w)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word: TAAC \n",
            "Cannot find this word\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \"\"\"\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-783f8600f4eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mall_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Word: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mwid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "35o95PNMqddn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "s0rlAD7Kqd-z",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a501e15e-2432-4c13-dd79-f3a8a2516fc2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525446033411,
          "user_tz": -180,
          "elapsed": 622,
          "user": {
            "displayName": "Otello 47",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110050518292405487974"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "i"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "jjykaIxfdG60",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "all_embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nNNpJsvwdG63",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}